{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8b42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "from pylib.tlsh_lib import *\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868abdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of Function\n",
    "def getResult(hashType, clusterType, labelList, clusterNumber):\n",
    "    data = tlist2cdata(hashList)\n",
    "    \n",
    "    d = {word: key for key, word in enumerate(set(labelList))}\n",
    "    labelList_id = [d[word] for word in labelList]\n",
    "    \n",
    "    outlierRemoveLabel = []\n",
    "    outlierRemoveID = []\n",
    "    outlierRemoveData = []\n",
    "    \n",
    "    for i in range(len(clusterNumber)):\n",
    "        if clusterNumber[i] >= 0:\n",
    "            outlierRemoveLabel.append(clusterNumber[i])\n",
    "            outlierRemoveID.append(labelList_id[i])\n",
    "            outlierRemoveData.append(data[i])\n",
    "            \n",
    "    #print(\"labelList_id=\", labelList_id)\n",
    "    #print(\"cluster labels=\",clusterNumber)\n",
    "    #print(\"outlierRemoveLabel =\", outlierRemoveLabel)\n",
    "    #print(\"outlierRemoveData =\", outlierRemoveData)\n",
    "    \n",
    "    # Number of decimal place for score\n",
    "    dp = 4 \n",
    "    \n",
    "    homo = round(metrics.homogeneity_score(outlierRemoveID, outlierRemoveLabel), dp)\n",
    "    silh1 = round(metrics.silhouette_score(data, clusterNumber, metric=sim), dp)\n",
    "    silh2 = round(metrics.silhouette_score(outlierRemoveData, outlierRemoveLabel, metric=sim), dp)\n",
    "    #cali = round(metrics.calinski_harabasz_score(outlierRemoveData, outlierRemoveLabel), dp)\n",
    "    #dav = round(metrics.davies_bouldin_score(outlierRemoveData, outlierRemoveLabel), dp)\n",
    "    \n",
    "    print(clusterType + \" ran in \" + str(end) + \" seconds\")\n",
    "    print(\"Homogeneity score =\",homo)\n",
    "    print(\"Silhouette score =\",silh1)\n",
    "    print(\"Silhouette score with Outlier Remove =\",silh2)\n",
    "    #print(\"Calinski harabasz score =\",cali)\n",
    "    #print(\"Davies bouldin score =\",dav)\n",
    "    #print(metrics.silhouette_samples(outlierRemoveData, outlierRemoveLabel, metric=sim))\n",
    "    print()\n",
    "    \n",
    "    result = {\"nSample\": int(len(tlist)),\n",
    "              \"Hash\": str(hashType),\n",
    "              \"Cluster\": str(clusterType),\n",
    "              \"nLabel\": int(nlabel),\n",
    "              \"nCluster\": int(max(clusterNumber)),\n",
    "              \"Time(s)\": float(end),\n",
    "              \"Homogeneity\": float(homo),\n",
    "              \"Silhouette\": float(silh2)\n",
    "              #\"Cal.\": float(cali),\n",
    "              #\"Dav.\": float(dav)\n",
    "             }\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8614b4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples is 323410\n",
      "Number of Unique Label is 537\n",
      "Example hash: T10263F782BC80EA22C7C01677FE6F518E331567D8E1EA32429D155FA07A8FC1B0D5B786\n"
     ]
    }
   ],
   "source": [
    "datafile = \"dataDir/mb_323425.csv\" #<-----Change this file size\n",
    "if (datafile == \"\"):\n",
    "    print(\"you must provide a datafile name (-f)\\n\")\n",
    "    sys.exit()\n",
    "\n",
    "tic = time.perf_counter()  # experiment time counter\n",
    "df = pd.DataFrame() #Result Table\n",
    "\n",
    "(path, file) = datafile.split(\"/\")  # save file path\n",
    "(filename, filetype) = file.split(\".\")  # save file type\n",
    "\n",
    "(tlist, [labelList, dateList, slist]) = tlsh_csvfile(datafile)  # return (tlshList, [labelList, dateList, ssdeepList])\n",
    "\n",
    "hashList = tlist\n",
    "\n",
    "print(\"Number of samples is \" + str(len(hashList)))\n",
    "print(\"Number of Unique Label is \" + str(len(set(labelList))))\n",
    "print(\"Example hash: \" + str(hashList[0]))\n",
    "nlabel = len(set(labelList))\n",
    "nClusters = [nlabel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5c5910e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agglomerative Clustering didn't work.\n",
      "Unable to allocate 779. GiB for an array with shape (323410, 323410) and data type float64\n"
     ]
    }
   ],
   "source": [
    "# Agglomerative Clustering\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    res = assignCluster(hashList, nlabel)\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"ac\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Agglomerative Clustering didn't work.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6d09953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbscan ran in 56910.678 seconds\n",
      "Homogeneity score = 0.9383\n",
      "Silhouette score = 0.4256\n",
      "Silhouette score with Outlier Remove = 0.7866\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DBSCAN\n",
    "from pylib.hac_lib import *\n",
    "try:\n",
    "    resetDistCalc()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    res = runDBSCAN(hashList, eps=30, min_samples=2, algorithm='auto')\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"dbscan\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "    nclusters = max(res.labels_)\n",
    "    nDistCalc = lookupDistCalc()\n",
    "    #print(\"nclusters is \" + str(nclusters))\n",
    "    #print(\"nDistCalc is \" + str(nDistCalc))\n",
    "\n",
    "    nClusters.append(nclusters)\n",
    "\n",
    "    #outfile = path + \"/output/\" + filename + \"_dbscan_out.txt\"\n",
    "    #outputClusters(outfile, hashList, res.labels_, labelList, quiet=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"DBSCAN didn't work.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67021186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hac-t ran in 3697.8187 seconds\n",
      "Homogeneity score = 0.9561\n",
      "Silhouette score = 0.4176\n",
      "Silhouette score with Outlier Remove = 0.7818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# HAC-T\n",
    "from pylib.hac_lib import *\n",
    "try:\n",
    "    hac_resetDistCalc()\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    res = HAC_T(datafile, CDist=30, step3=0, outfname=\"tmp.txt\", cenfname=\"tmp2.txt\")\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"hac-t\", labelList, res)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "    nclusters = max(res)\n",
    "    nDistCalc = hac_lookupDistCalc()\n",
    "    #print(\"nclusters is \" + str(nclusters))\n",
    "    #print(\"nDistCalc is \" + str(nDistCalc))\n",
    "\n",
    "    nClusters.append(nclusters)\n",
    "\n",
    "    #outfile = path + \"/output/\" + filename + \"_hac-t_out.txt\"\n",
    "    #outputClusters(outfile, hashList, res, labelList, quiet=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"HAC-T didn't work.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5545bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTICS\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    res = runOPTICS(hashList, min_samples=2)\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"optics\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"OPTICS didn't work.\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b370d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# KMeans\n",
    "for i in nClusters:\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        res = runKMean(hashList, n_clusters=i)\n",
    "        end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "        dict = getResult(\"tlsh\", \"kmeans\", labelList, res.labels_)\n",
    "        df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"KMeans didn't work.\")\n",
    "        print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12beb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# BIRCH\n",
    "for i in nClusters:\n",
    "    try:\n",
    "        start = time.perf_counter()\n",
    "        res = runBIRCH(hashList, n_clusters=i)\n",
    "        end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "        dict = getResult(\"tlsh\", \"birch\", labelList, res.labels_)\n",
    "        df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"BIRCH didn't work.\")\n",
    "        print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c09c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Affinity Propagation\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    res = runAffinityPropagation(hashList, random_state=5)\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"ap\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Affinity Propagation didn't work.\")\n",
    "    print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf509738",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Mean Shift\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    res = runMeanShift(hashList, bandwidth=5)\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"ms\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Mean Shift didn't work.\")\n",
    "    print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0252f43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Spectral Clustering\n",
    "try:\n",
    "    start = time.perf_counter()\n",
    "    res = runSpectral(hashList, n_clusters=nlabel)\n",
    "    end = round(time.perf_counter() - start, 4)\n",
    "\n",
    "    dict = getResult(\"tlsh\", \"sp\", labelList, res.labels_)\n",
    "    df = pd.concat((df, pd.DataFrame([dict])), ignore_index=True)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Spectral Clustering didn't work.\")\n",
    "    print(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "outfile = path + \"/output/\" + filename + \"_Tlsh_result.csv\"\n",
    "df.to_csv(outfile, index=False)\n",
    "\n",
    "toc = round(time.perf_counter() - tic, 4)\n",
    "\n",
    "print(\"All code ran in \" + str(toc) + \" seconds\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2bc51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
